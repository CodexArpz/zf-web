<h2>ZF-690: Large file indexing problem</h2>

<dl class="metadata">
    <dt>Issue Type:</dt>
    <dd>Bug</dd>

    <dt>Created:</dt>
    <dd>2006-12-26T23:01:59.000+0000</dd>

    <dt>Last Updated:</dt>
    <dd>2007-07-05T14:43:07.000+0000</dd>

    <dt>Status:</dt>
    <dd>Resolved</dd>

    <dt>Fix version(s):</dt>
    <dd><ul></ul></dd>

    <dt>Reporter:</dt>
    <dd>
        None
                    </dd>

    <dt>Assignee:</dt>
    <dd>
                Alexander Veremyev (alexander)
            </dd>

    <dt>Tags:</dt>
    <dd><ul>        <li>Zend_Search_Lucene</li>
        </ul></dd>

    <dt>Related issues:</dt>
    <dd><ul>
    </ul></dd>

    <dt>Attachments:</dt>
    <dd><ul>
        <li><a href="/issues/secure/attachment/10193/oracle_error_messages.zip">oracle_error_messages.zip</a></li>
        </ul></dd>
</dl>

<div class="description">
    <h3>Description</h3>

    <div class="body">
        <p>The engine can't index my large text file(it's size is 2.852.000 bytes, overall words count is about 460 thousands words, no special conents, just text only). 
It takes all memory(about 1 Gb memory and swap) and hangs my PC. 
I'm using Win2k3 server, IIS6, PHP-5.1.6 CGI module and Zend Framework 0.6.0 version.</p>

    </div>
</div>

<div class="comments">
    <h3>Comments</h3>

    <div class="comment">
        <p class="metadata">Posted by Alexander Veremyev (alexander) on 2006-12-27T00:24:20.000+0000</p> 
        <div class="body">
            <p>Do you index one document per script execution?</p>

<p>If no, please try to use $index-&gt;commit(); after each documentAdd() call.
(Or set <em>MaxBufferedDocs</em> to 1 - $index-&gt;setMaxBufferedDocs(1))</p>

<p>What is <em>memory_limit</em> value in php.ini?</p>

<p>That would be also good, if you can attach this file to the issue (or send it to me).</p>

        </div>
    </div>
        <div class="comment">
        <p class="metadata">Posted by Andrew Kharchenko (kharchenkoav) on 2006-12-27T00:51:25.000+0000</p> 
        <div class="body">
            <p>The $index-&gt;setMaxBufferedDocs(1) is already set in my code.
The $index-&gt;commit(); is used after each documentAdd() call.
But the problem is the all memory used within the documentAdd() call. The script hangs up before commit() is executed. 
The memory_limit is set to 128M but actually I'm using now the command line php interpreter which has no memory limits.
The file is attached to the report.</p>

        </div>
    </div>
        <div class="comment">
        <p class="metadata">Posted by Alexander Veremyev (alexander) on 2006-12-27T01:38:20.000+0000</p> 
        <div class="body">
            <p>I got the same result.</p>

        </div>
    </div>
        <div class="comment">
        <p class="metadata">Posted by Alexander Veremyev (alexander) on 2006-12-27T01:39:10.000+0000</p> 
        <div class="body">
            <p>So attached file is helpful. I'll check, how it can be managed.</p>

        </div>
    </div>
        <div class="comment">
        <p class="metadata">Posted by Alexander Veremyev (alexander) on 2006-12-27T04:24:01.000+0000</p> 
        <div class="body">
            <p>Fixed. Take an SVN version.
Stream mode tokenization API is added to analyzer.</p>

<p>It still need a lot of memory for indexing, but much smaller.
It took ~35Mb with PHP V5.1.4 and ~70Mb with PHP V5.2</p>

<p>PS Command line php also uses memory limit option from php.ini</p>

        </div>
    </div>
        <div class="comment">
        <p class="metadata">Posted by Andrew Kharchenko (kharchenkoav) on 2006-12-28T03:40:30.000+0000</p> 
        <div class="body">
            <p>Thanks. The SVN version solved my problem.</p>

<p>P.S. Thanks in advance.</p>

        </div>
    </div>
    </div>

